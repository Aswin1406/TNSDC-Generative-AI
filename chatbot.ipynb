{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra|\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3d604755-5e26-44f1-9339-b3c074eb0b15","_cell_guid":"727ac648-d64f-47d1-9afe-25d3ecf772a5","scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T09:02:07.985277Z","iopub.execute_input":"2022-05-30T09:02:07.985660Z","iopub.status.idle":"2022-05-30T09:02:08.090313Z","shell.execute_reply.started":"2022-05-30T09:02:07.985533Z","shell.execute_reply":"2022-05-30T09:02:08.089630Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mental-health-faq-for-chatbot/Mental_Health_FAQ.csv\n/kaggle/input/simple-dialogs-for-chatbot/dialogs.txt\n/kaggle/input/conversational-question-answering-dataset-coqa/coqa-dev-v1.0.json\n/kaggle/input/conversational-question-answering-dataset-coqa/coqa-train-v1.0.json\n/kaggle/input/pythonquestions/Answers.csv\n/kaggle/input/pythonquestions/Questions.csv\n/kaggle/input/pythonquestions/Tags.csv\n/kaggle/input/askreddit-questions-and-answers/askreddit.db\n/kaggle/input/askreddit-questions-and-answers/reddit_questions.csv\n/kaggle/input/askreddit-questions-and-answers/reddit_answers.csv\n/kaggle/input/askreddit-questions-and-answers/reddit_answers_long.csv\n/kaggle/input/askreddit-troll-questions/our_competition_train (1).csv\n/kaggle/input/askreddit-troll-questions/our_competition_test.csv\n/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\n/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv\n/kaggle/input/chatbot-dataset-topical-chat/topical_chat.csv\n/kaggle/input/human-conversation-training-data/human_chat.txt\n/kaggle/input/qa-jokes/jokes.csv\n/kaggle/input/ten-million-reddit-answers/ten-million-reddit-answers.csv\n/kaggle/input/ten-million-reddit-answers/ten-million-reddit-answers-questions.csv\n/kaggle/input/150k-python-dataset/py150.tar_1\n/kaggle/input/stacksample/Answers.csv\n/kaggle/input/stacksample/Questions.csv\n/kaggle/input/stacksample/Tags.csv\n/kaggle/input/amazon-questionanswer-dataset/multi_questions.csv\n/kaggle/input/amazon-questionanswer-dataset/single_qna.csv\n/kaggle/input/amazon-questionanswer-dataset/multi_answers.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stacksample/Tags.csv')\ntag = []\n# print(len(df['Tag']))\n# print(len(df['Id']))\nfor i in range(0, len(df)):\n    if df['Tag'][i] == 'python':\n        tag.append(df['Id'][i])\n        \nprint(len(tag))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:02:14.875165Z","iopub.execute_input":"2022-05-30T09:02:14.875563Z","iopub.status.idle":"2022-05-30T09:02:35.686450Z","shell.execute_reply.started":"2022-05-30T09:02:14.875531Z","shell.execute_reply":"2022-05-30T09:02:35.685526Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"64601\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stacksample/Questions.csv', encoding = 'ISO-8859-1')\nquestions = []\nanswers = []\nprint(len(df['Id'].unique()))\nfor i in range(0, 100000):\n    if df['Id'][i] in tag:\n        questions.append(df['OwnerUserId'][i])\n        questions.append(df['Body'][i])\n        \nprint(len(questions))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:03:20.015159Z","iopub.execute_input":"2022-05-30T09:03:20.015439Z","iopub.status.idle":"2022-05-30T09:05:53.777021Z","shell.execute_reply.started":"2022-05-30T09:03:20.015412Z","shell.execute_reply":"2022-05-30T09:05:53.776187Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1264216\n7802\n","output_type":"stream"}]},{"cell_type":"code","source":"questions[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:06:49.355721Z","iopub.execute_input":"2022-05-30T09:06:49.356026Z","iopub.status.idle":"2022-05-30T09:06:49.365961Z","shell.execute_reply.started":"2022-05-30T09:06:49.355991Z","shell.execute_reply":"2022-05-30T09:06:49.365170Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[912.0,\n \"<p>This is a difficult and open-ended question I know, but I thought I'd throw it to the floor and see if anyone had any interesting suggestions.</p>\\n\\n<p>I have developed a code-generator that takes our python interface to our C++ code (generated via SWIG) and generates code needed to expose this as WebServices.  When I developed this code I did it using TDD, but I've found my tests to be brittle as hell.  Because each test essentially wanted to verify that for a given bit of input code (which happens to be a C++ header) I'd get a given bit of outputted code I wrote a small engine that reads test definitions from XML input files and generates test cases from these expectations.</p>\\n\\n<p>The problem is I dread going in to modify the code at all.  That and the fact that the unit tests themselves are a: complex, and b: brittle.</p>\\n\\n<p>So I'm trying to think of alternative approaches to this problem, and it strikes me I'm perhaps tackling it the wrong way.  Maybe I need to focus more on the outcome, IE: does the code I generate actually run and do what I want it to, rather than, does the code look the way I want it to.</p>\\n\\n<p>Has anyone got any experiences of something similar to this they would care to share?</p>\\n\",\n 394.0,\n '<p>I\\'m creating an ZIP file with ZipFile in Python 2.5, it works ok so far:</p>\\n\\n<pre><code>import zipfile, os\\n\\nlocfile = \"test.txt\"\\nloczip = os.path.splitext (locfile)[0] + \".zip\"\\nzip = zipfile.ZipFile (loczip, \"w\")\\nzip.write (locfile)\\nzip.close()\\n</code></pre>\\n\\n<p>but I couldn\\'t find how to encrypt the files in the ZIP file.\\nI could use system and call PKZIP -s, but I suppose there must be a more \"Pythonic\" way.  I\\'m looking for an open source solution.</p>\\n',\n 745.0,\n '<p>I have a bunch of files (TV episodes, although that is fairly arbitrary) that I want to check match a specific naming/organisation scheme..</p>\\n\\n<p>Currently: I have three arrays of regex, one for valid filenames, one for files missing an episode name, and one for valid paths.</p>\\n\\n<p>Then, I loop though each valid-filename regex, if it matches, append it to a \"valid\" dict, if not, do the same with the missing-ep-name regexs, if it matches this I append it to an \"invalid\" dict with an error code (2:\\'missing epsiode name\\'), if it matches neither, it gets added to invalid with the \\'malformed name\\' error code.</p>\\n\\n<p>The current code can be found <a href=\"http://github.com/dbr/checktveps/tree/8a6dc68ad61e684c8d8f0ca1dc37a22d1c51aa82/2checkTvEps.py\" rel=\"nofollow\">here</a></p>\\n\\n<p>I want to add a rule that checks for the presence of a folder.jpg file in each directory, but to add this would make the code substantially more messy in it\\'s current state.. </p>\\n\\n<p>How could I write this system in a more expandable way?</p>\\n\\n<p>The rules it needs to check would be..</p>\\n\\n<ul>\\n<li>File is in the format <code>Show Name - [01x23] - Episode Name.avi</code> or <code>Show Name - [01xSpecial02] - Special Name.avi</code> or <code>Show Name - [01xExtra01] - Extra Name.avi</code></li>\\n<li>If filename is in the format <code>Show Name - [01x23].avi</code> display it a \\'missing episode name\\' section of the output</li>\\n<li>The path should be in the format <code>Show Name/season 2/the_file.avi</code> (where season 2 should be the correct season number in the filename)</li>\\n<li>each <code>Show Name/season 1/</code> folder should contain \"folder.jpg\"</li>\\n</ul>\\n\\n<p>.any ideas? While I\\'m trying to check TV episodes, this concept/code should be able to apply to many things..</p>\\n\\n<p>The only thought I had was a list of dicts in the format:</p>\\n\\n<pre><code>checker = [\\n{\\n    \\'name\\':\\'valid files\\',\\n    \\'type\\':\\'file\\',\\n    \\'function\\':check_valid(), # runs check_valid() on all files\\n    \\'status\\':0 # if it returns True, this is the status the file gets\\n}\\n</code></pre>\\n',\n 242853.0,\n \"<p>I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.</p>\\n\\n<p>From what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?</p>\\n\\n<p>Where is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python.</p>\\n\",\n 3561.0,\n \"<p>A reliable coder friend told me that Python's current multi-threading implementation is seriously buggy - enough to avoid using altogether.  What can said about this rumor?</p>\\n\"]"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stacksample/Answers.csv', encoding = 'ISO-8859-1')\nanswers = []\nprint(len(df['Id'].unique()))\nfor i in range(0, 300000):\n    if df['Id'][i] in tag:\n        answers.append(df['ParentId'][i])\n        answers.append(df['Body'][i])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:13:26.154815Z","iopub.execute_input":"2022-05-30T09:13:26.155233Z","iopub.status.idle":"2022-05-30T09:18:34.024025Z","shell.execute_reply.started":"2022-05-30T09:13:26.155201Z","shell.execute_reply":"2022-05-30T09:18:34.023161Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2014516\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/2268299595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ParentId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"answers[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:13:06.280809Z","iopub.execute_input":"2022-05-30T09:13:06.281093Z","iopub.status.idle":"2022-05-30T09:13:06.286713Z","shell.execute_reply.started":"2022-05-30T09:13:06.281061Z","shell.execute_reply":"2022-05-30T09:13:06.286132Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"markdown","source":"***Human Conversation dialogs***","metadata":{"_uuid":"679cad90-7b7f-4ed0-9b4e-17a032dc890a","_cell_guid":"078d68cc-8b1d-4df1-a25b-946beb4e9b09","trusted":true}},{"cell_type":"code","source":"dialogs = []\nwith open('/kaggle/input/human-conversation-training-data/human_chat.txt', 'r') as w:\n    for line in w:\n        try:\n            dialogs.append(line.split(': ')[1].split(\"\\n\")[0])\n        except:\n            print('done')\n        \nlen(dialogs)\ndialogs","metadata":{"_uuid":"72d3600d-927b-4da4-aec4-27fe98f1d030","_cell_guid":"60ae89f7-dac9-4df8-908a-4ccb50ac4655","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dialogs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Mental Health Question Answers***","metadata":{"_uuid":"add7f812-3e9a-4542-b31b-f99c513f257c","_cell_guid":"7cfa14c7-c8f7-4058-9a56-04338ea466a3","trusted":true}},{"cell_type":"code","source":"mh = pd.read_csv('/kaggle/input/mental-health-faq-for-chatbot/Mental_Health_FAQ.csv')\nmh.head","metadata":{"_uuid":"a3568942-6e7e-4af8-a005-52df8f1b17f7","_cell_guid":"80657e2d-dc76-46f1-a2e3-fe7b6674ce60","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = list(zip(mh.Questions, mh.Answers))\nqna = []\nfor i in data:\n    data1 = []\n    data1.append(i[0])\n    data1.append(i[1])\n    qna.append(data1)\n    \nprint(list(mh.Answers))","metadata":{"_uuid":"03435608-7dfe-41a2-ad49-120a76a8f9d3","_cell_guid":"c6cb6eb6-ae68-4589-bdb2-63a09802fb3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Empathetic Dialogues***","metadata":{"_uuid":"1ab2b374-1eb5-4294-99a0-d30dad986162","_cell_guid":"74fa7bab-b531-427f-b3d6-4e1744200316","trusted":true}},{"cell_type":"code","source":"ed = pd.read_csv('/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv')\ned.head","metadata":{"_uuid":"23456ff1-7de0-4fac-b522-fab14024e0d2","_cell_guid":"aee62da1-67ec-464f-b0b4-2d0099c1a740","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ed.columns","metadata":{"_uuid":"37816555-8958-46db-864c-76f88ee047ad","_cell_guid":"c3578c52-f561-4798-8a60-4acac1d04d01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emapthetic_dialogs = []\nfor i in ed.empathetic_dialogues:\n    try: \n        emapthetic_dialogs.append(i.split('\\nAgent :')[0].split('Customer :')[1])\n    except:\n        continue\n    \nlen(emapthetic_dialogs)\nemapthetic_dialogs","metadata":{"_uuid":"e885ca7e-3c4e-4960-b9aa-bdfa0977e171","_cell_guid":"4d0462ae-8013-46a4-ba8d-928154cece72","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Topical Chat***","metadata":{"_uuid":"3ccc9a5a-56af-4c6a-8712-f056a82e984f","_cell_guid":"b08f3431-bfb5-4601-abb3-301828812c98","trusted":true}},{"cell_type":"code","source":"tc = pd.read_csv('/kaggle/input/chatbot-dataset-topical-chat/topical_chat.csv')\ntc.head","metadata":{"_uuid":"1e5f4861-3265-4a35-bd7f-a8a43a0a3866","_cell_guid":"3784fc51-a295-45e5-a444-b99fe741deb4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tc_dialogs = list(tc.message)\ntc_dialogs[:50]\nlist(tc.message)","metadata":{"_uuid":"d7eb1964-c6f1-4c7b-a344-a5653533efba","_cell_guid":"74e62805-f600-4684-946c-758d5bd79307","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tc_dialogs)","metadata":{"_uuid":"b0567350-51c7-4b50-a5c3-eab183f57bf8","_cell_guid":"e151ef30-0761-4fa7-949e-f71fb558eb9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Ask Reddit Question and Answers***","metadata":{"_uuid":"630898d3-13a4-4986-b430-d1e64d6f0a3e","_cell_guid":"a66a655f-97d8-44a6-b3ee-f71d427afda0","trusted":true}},{"cell_type":"code","source":"# ar = pd.read_csv('/kaggle/input/askreddit-questions-and-answers/reddit_questions.csv')","metadata":{"_uuid":"f7ff5cfa-50b6-4b17-8bfb-1a237da8c924","_cell_guid":"2d64f432-ddd2-41f2-8249-6c7bf25c1bba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Jeopardy questions***","metadata":{"_uuid":"36f38a73-99ae-49d1-ae4e-e95f8f989765","_cell_guid":"d64e66e0-467f-4c61-bca3-e61fbe27613a","trusted":true}},{"cell_type":"code","source":"jq = pd.read_csv('/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv')\njq.head","metadata":{"_uuid":"03c68748-28f8-49f8-ab9f-67aa25918d78","_cell_guid":"493a5cf4-6100-40ac-a705-d3f986f71d2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jq.describe","metadata":{"_uuid":"1ff317a8-153b-44f4-8af8-b73a20b7e113","_cell_guid":"4e01175b-fa8a-49f6-8e97-8162095283eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jq.columns","metadata":{"_uuid":"594a2889-094c-416c-829c-b47cf5eb6ced","_cell_guid":"d7d9477d-50e1-4ede-800d-28eed72cf632","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jq[' Question']","metadata":{"_uuid":"103fd58d-154e-41ef-9b85-22630f071e48","_cell_guid":"ec7abc15-e233-4925-ad2f-950c24c22ec9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jq[' Answer']","metadata":{"_uuid":"e1a340d1-7a64-402e-94ce-005638bc0189","_cell_guid":"1f5957f1-7dce-4685-8b07-fc14178feaac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = list(zip(jq[' Question'], jq[' Answer']))\njqna = []\nfor i in data:\n    data1 = []\n    data1.append(str(i[0]))\n    data1.append(str(i[1]))\n    jqna.append(data1)\n    \nlen(jqna)","metadata":{"_uuid":"bf3fd106-8e3f-48cb-bd6d-63c631252b77","_cell_guid":"20a35c56-6f38-4fac-bd35-493a94d43519","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***150k PYTHON SOURCE CODE DATASET***","metadata":{"_uuid":"f184db04-0306-413f-b26e-2480347f10d0","_cell_guid":"9ab00795-974e-4d83-b19e-406aba5632d8","trusted":true}},{"cell_type":"code","source":"import tarfile\ntar = tarfile.open(\"/kaggle/input/150k-python-dataset/py150.tar_1\")\ntar.extractall()\ntar.close()","metadata":{"_uuid":"ac576857-1320-4228-a59e-ecf61d45444e","_cell_guid":"a12ffd0d-cb2c-4f76-824b-4334d37003a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"_uuid":"45fb2a9c-4f6a-45a3-adeb-ae99a2ce8ffa","_cell_guid":"99969c4e-a660-4727-8a39-fd780aad9e07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport json as json\nimport ast\n\ndef PrintUsage():\n    sys.stderr.write(\"\"\"\nUsage:\n    parse_python.py <file>\n\n\"\"\")\n    exit(1)\n\ndef read_file_to_string(filename):\n    f = open(filename, 'rt')\n    s = f.read()\n    f.close()\n    return s\n\ndef parse_file(filename):\n    global c, d\n    tree = ast.parse(read_file_to_string(filename), filename)\n    \n    json_tree = []\n    def gen_identifier(identifier, node_type = 'identifier'):\n        pos = len(json_tree)\n        json_node = {}\n        json_tree.append(json_node)\n        json_node['type'] = node_type\n        json_node['value'] = identifier\n        return pos\n    \n    def traverse_list(l, node_type = 'list'):\n        pos = len(json_tree)\n        json_node = {}\n        json_tree.append(json_node)\n        json_node['type'] = node_type\n        children = []\n        for item in l:\n            children.append(traverse(item))\n        if (len(children) != 0):\n            json_node['children'] = children\n        return pos\n        \n    def traverse(node):\n        pos = len(json_tree)\n        json_node = {}\n        json_tree.append(json_node)\n        json_node['type'] = type(node).__name__\n        children = []\n        if isinstance(node, ast.Name):\n            json_node['value'] = node.id\n        elif isinstance(node, ast.Num):\n            json_node['value'] = unicode(node.n)\n        elif isinstance(node, ast.Str):\n            json_node['value'] = node.s.decode('utf-8')\n        elif isinstance(node, ast.alias):\n            json_node['value'] = unicode(node.name)\n            if node.asname:\n                children.append(gen_identifier(node.asname))\n        elif isinstance(node, ast.FunctionDef):\n            json_node['value'] = unicode(node.name)\n        elif isinstance(node, ast.ClassDef):\n            json_node['value'] = unicode(node.name)\n        elif isinstance(node, ast.ImportFrom):\n            if node.module:\n                json_node['value'] = unicode(node.module)\n        elif isinstance(node, ast.Global):\n            for n in node.names:\n                children.append(gen_identifier(n))\n        elif isinstance(node, ast.keyword):\n            json_node['value'] = unicode(node.arg)\n        \n\n        # Process children.\n        if isinstance(node, ast.For):\n            children.append(traverse(node.target))\n            children.append(traverse(node.iter))\n            children.append(traverse_list(node.body, 'body'))\n            if node.orelse:\n                children.append(traverse_list(node.orelse, 'orelse'))\n        elif isinstance(node, ast.If) or isinstance(node, ast.While):\n            children.append(traverse(node.test))\n            children.append(traverse_list(node.body, 'body'))\n            if node.orelse:\n                children.append(traverse_list(node.orelse, 'orelse'))\n        elif isinstance(node, ast.With):\n            children.append(traverse(node.context_expr))\n            if node.optional_vars:\n                children.append(traverse(node.optional_vars))\n            children.append(traverse_list(node.body, 'body'))\n        elif isinstance(node, ast.TryExcept):\n            children.append(traverse_list(node.body, 'body'))\n            children.append(traverse_list(node.handlers, 'handlers'))\n            if node.orelse:\n                children.append(traverse_list(node.orelse, 'orelse'))\n        elif isinstance(node, ast.TryFinally):\n            children.append(traverse_list(node.body, 'body'))\n            children.append(traverse_list(node.finalbody, 'finalbody'))\n        elif isinstance(node, ast.arguments):\n            children.append(traverse_list(node.args, 'args'))\n            children.append(traverse_list(node.defaults, 'defaults'))\n            if node.vararg:\n                children.append(gen_identifier(node.vararg, 'vararg'))\n            if node.kwarg:\n                children.append(gen_identifier(node.kwarg, 'kwarg'))\n        elif isinstance(node, ast.ExceptHandler):\n            if node.type:\n                children.append(traverse_list([node.type], 'type'))\n            if node.name:\n                children.append(traverse_list([node.name], 'name'))\n            children.append(traverse_list(node.body, 'body'))\n        elif isinstance(node, ast.ClassDef):\n            children.append(traverse_list(node.bases, 'bases'))\n            children.append(traverse_list(node.body, 'body'))\n            children.append(traverse_list(node.decorator_list, 'decorator_list'))\n        elif isinstance(node, ast.FunctionDef):\n            children.append(traverse(node.args))\n            children.append(traverse_list(node.body, 'body'))\n            children.append(traverse_list(node.decorator_list, 'decorator_list'))\n        else:\n            # Default handling: iterate over children.\n            for child in ast.iter_child_nodes(node):\n                if isinstance(child, ast.expr_context) or isinstance(child, ast.operator) or isinstance(child, ast.boolop) or isinstance(child, ast.unaryop) or isinstance(child, ast.cmpop):\n                    # Directly include expr_context, and operators into the type instead of creating a child.\n                    json_node['type'] = json_node['type'] + type(child).__name__\n                else:\n                    children.append(traverse(child))\n                \n        if isinstance(node, ast.Attribute):\n            children.append(gen_identifier(node.attr, 'attr'))\n                \n        if (len(children) != 0):\n            json_node['children'] = children\n        return pos\n    \n    traverse(tree)\n    return json.dumps(json_tree, separators=(',', ':'), ensure_ascii=False)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        PrintUsage()\n    try:\n        print('file')\n        #print(parse_file(sys.argv[1]))\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        pass","metadata":{"_uuid":"a0ead7c4-5fb6-46b9-b14c-8dfe21584232","_cell_guid":"518260fb-6b70-422c-98e8-5d4fd574f6a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Python Questions Dataset***","metadata":{"_uuid":"480a8f9d-19da-4b41-af9c-04285b8544bf","_cell_guid":"93bcaacc-908e-4da5-b92d-b2fe5390663a","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (GPT2Config,GPT2LMHeadModel,GPT2Tokenizer)\nimport torch\nfrom string import punctuation as pnc\nfrom collections import Counter\nfrom scipy import spatial\nfrom bs4 import BeautifulSoup\nfrom tqdm.notebook import tqdm\nimport torch\nimport pylab as pl\npd.set_option('display.max_colwidth', -1)","metadata":{"_uuid":"d0ea4cf3-7c49-4671-81d3-dd178fb2cdc1","_cell_guid":"dd6bd0f3-fadd-4abd-a10c-86dde231c35b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"_uuid":"49a9bf28-1c9f-45cd-af7c-d1ab8c7d361d","_cell_guid":"94d352f6-56b8-46a1-a3aa-ec50e04453f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions = pd.read_csv(\"/kaggle/input/pythonquestions/Questions.csv\", encoding = \"ISO-8859-1\")\nprint(len(questions))\ndisplay(questions.head(5))","metadata":{"_uuid":"d0d3c388-5b08-4b4a-9e6d-7d308d434fd9","_cell_guid":"1e653a0e-dc38-4a79-9adf-5f1c65d6e37b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of unique Questions : \", questions['Id'].nunique())","metadata":{"_uuid":"1b3d1876-d835-4b73-8785-00cb729c6249","_cell_guid":"6cafb601-0014-4f48-b592-a8f6cf356ea0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions['qLen'] = questions['Title'].apply(lambda x : len(x.split(\" \")))\nquestions['qBodyLen'] = questions['Body'].apply(lambda x : len(x.split(\" \")))","metadata":{"_uuid":"ea269e8d-6aa7-4cb4-9d20-4b74bcf47ea2","_cell_guid":"eb0021b9-19c0-4fe1-985d-37fc7a6af1d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"_uuid":"0576602a-1949-463f-8a0d-1d074cd28590","_cell_guid":"22e67e64-196a-44ce-9ae9-91f8c585c555","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions['qLen'].hist(bins=35)\nplt.title(\"No. of words in Title\")","metadata":{"_uuid":"1c7337a3-a3a7-4ead-8c77-96eeb4e1b670","_cell_guid":"b22ce43b-c166-4db6-8d43-4e89b0d697b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions[questions['qBodyLen']<500]['qBodyLen'].hist(bins=100)\nplt.title(\"No. of words in Body\")","metadata":{"_uuid":"46970430-290e-46af-a6bf-7d5c1dd5d57c","_cell_guid":"00b40dff-ef8e-4cd7-8014-00aae13355b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getWordCloud(df,col):\n  comment_words = '' \n  stopwords = set(STOPWORDS) \n    \n  for val in tqdm(df[col]): \n        \n      val = str(val) \n      tokens = val.split() \n        \n      for i in range(len(tokens)): \n          tokens[i] = tokens[i].lower() \n        \n      comment_words += \" \".join(tokens)+\" \"\n    \n  wordcloud = WordCloud(width = 800, height = 800, \n                  background_color ='white', \n                  stopwords = stopwords, \n                  min_font_size = 10).generate(comment_words) \n    \n                       \n  plt.figure(figsize = (5, 5), facecolor = None) \n  plt.imshow(wordcloud) \n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0) \n    \n  plt.show()","metadata":{"_uuid":"03ae8278-ff2b-4778-9208-2f98df51269a","_cell_guid":"ac2699a2-e223-45e5-86a1-3aed522ee1fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getWordCloud(questions,'Title')","metadata":{"_uuid":"d9d16946-220e-42ea-a3ae-3ce8418d55d0","_cell_guid":"f8f17bfb-4eeb-4725-91ab-fa194b206bc9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop = stopwords.words('english')\ndef preprocess(df, col):\n  df['preprocessed'+col] = df[col].apply(lambda x : \" \".join([word for word in x.split(\" \") if word not in stop]))\n  df['preprocessed'+col] = df['preprocessed'+col].str.replace('[^a-zA-Z0-9 ]', '')\n  df['preprocessed'+col] = df['preprocessed'+col].str.lower()\n  return df","metadata":{"_uuid":"0fb03b76-e637-4336-9ea8-f224b9aea974","_cell_guid":"bc1d9dc0-d133-4290-9fc6-acef43a2a274","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions = preprocess(questions, 'Title')","metadata":{"_uuid":"b5008545-a6b3-46ac-bfb9-c20d07366c60","_cell_guid":"ea0aae65-7144-4dbe-96f6-b92ec9106b35","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = pd.read_csv(\"/kaggle/input/pythonquestions/Tags.csv\", encoding = \"ISO-8859-1\")\nprint(len(tags))\ndisplay(tags.head(5))","metadata":{"_uuid":"945a90c1-0e15-4fb9-ad1d-723401a66034","_cell_guid":"beb3dcd2-5b98-469e-a1e1-f73a4b7e5248","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of unique Tags : \", tags['Tag'].nunique())","metadata":{"_uuid":"3e7f18cb-79af-4ab2-96ab-7dac23f01ca2","_cell_guid":"e995dd66-1095-4812-9a56-5c85c98be7a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\ntags[tags['Tag']!='python']['Tag'].value_counts().sort_values(ascending = False)[:20].plot(ax=ax, kind='bar')","metadata":{"_uuid":"174a9e32-cf22-4d9c-8b37-1d53b571921e","_cell_guid":"697b84d0-02a9-4af5-9132-72fbbe5d30b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\nmodel = model_class.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"_uuid":"eae0449c-b57c-4d56-90b0-d308db77f95d","_cell_guid":"7d6e62d6-b920-4271-a273-72d143d7c73e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessedTitle = questions['preprocessedTitle'].values\nQID = questions['Id'].values\nprint(len(preprocessedTitle), len(QID))","metadata":{"_uuid":"21c9671e-82f6-42fc-b72c-0969d1195b23","_cell_guid":"732b9c1d-cfab-42cc-8d46-24f162eed01c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodedpreprocessedTitle = tokenizer.batch_encode_plus(preprocessedTitle)['input_ids']\nprint(len(encodedpreprocessedTitle))","metadata":{"_uuid":"ae14a102-05c9-4362-a559-425678b72cee","_cell_guid":"f5886bcc-d7cb-4151-a09e-e8e5eb60a993","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddigs = model.transformer.wte\nprint(\"Shape of embedding matrix : \",embeddigs.weight.shape)\nprint(\"Type of embedding matrix : \", type(embeddigs))","metadata":{"_uuid":"d782ad4e-88ef-4f0e-a88f-ce32d5a6231a","_cell_guid":"a6743a0e-bff1-4387-b9a8-fbef5b7d78a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TitleEmbeddingList = []\nQIDList = []\nfor idx, (qid, encodedTitle) in tqdm(enumerate(zip(QID, encodedpreprocessedTitle))):\n  if len(encodedTitle) > 0 :\n    embeddedTitle = embeddigs(torch.tensor(encodedTitle).to(torch.int64)).mean(axis=0)\n    TitleEmbeddingList.append(embeddedTitle)\n    QIDList.append(qid)","metadata":{"_uuid":"3f859d52-71b9-4e97-8e54-8db246e0565c","_cell_guid":"7a899305-cca5-41c9-87c6-bc60350a2077","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numQ = len(TitleEmbeddingList)\nembedDim = len(TitleEmbeddingList[0])\nprint(\"Number of Titles : \",numQ,\" and Length of vector of each Title : \",embedDim)","metadata":{"_uuid":"2473a41b-1d96-45e6-9900-acca3702cd35","_cell_guid":"1e252e3e-4a5b-4a9d-9b33-fd151bdab3dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TitleEmbeddingTensor = torch.cat(TitleEmbeddingList, dim=0)\nTitleEmbeddingTensor = torch.reshape(TitleEmbeddingTensor, (numQ, embedDim))\nprint(\"Shape of TitleEmbeddingTensor : \",TitleEmbeddingTensor.shape)\nprint(\"Type of TitleEmbeddingTensor : \", type(TitleEmbeddingTensor))","metadata":{"_uuid":"fcc22dc5-aa0d-4e5c-be15-36d39a9e15ae","_cell_guid":"8d36414e-1666-4955-9f7d-7201ade64996","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocesstext(text):\n  text =  \" \".join([word for word in text.split(\" \") if word not in stop])\n  text = re.sub(r'[^a-zA-Z0-9 ]','',text)\n  text = text.lower()\n  return text","metadata":{"_uuid":"c14866f1-4450-497d-8966-0c80b047c55f","_cell_guid":"0a293dba-bbf4-4d03-80e2-9aff9db9c0b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMostSimilarQuestionsIdx(K, a, b):\n  a_norm = a / a.norm(dim=1)[:, None]\n  b_norm = b / b.norm(dim=1)[:, None]\n  res = torch.mm(a_norm, b_norm.transpose(0,1)).squeeze(0)\n  res = res.tolist()\n  mostSimIdx = sorted(range(len(res)), key=lambda x: res[x])[-K:]\n  return mostSimIdx","metadata":{"_uuid":"7f04290d-06ee-4ca2-b560-5d275bb99421","_cell_guid":"57fde280-d5d9-4ecc-98de-c3609f57eb2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMostSimilarQuestions(K, input, QuestionDF, QIDList):\n  input = input\n  preprocessedinput = preprocesstext(input)\n  inputEncoded = tokenizer.batch_encode_plus([preprocessedinput])['input_ids']\n  inputEmbedded = embeddigs(torch.tensor(inputEncoded).to(torch.int64)).squeeze(0).mean(axis=0).unsqueeze(0)\n  mostSimilarIdx = getMostSimilarQuestionsIdx(K, inputEmbedded, TitleEmbeddingTensor)\n  mostSimilarIdx.reverse()\n  print(\"Most similar \",K, \" questions : \")\n  for idx, simidx in enumerate(mostSimilarIdx):\n    IDQ = QuestionDF[QuestionDF['Id']==QIDList[simidx]][['Id','Title']].values\n    parentId = IDQ[0][0]\n    simQuestion = IDQ[0][1]\n    print((idx+1), \"Question Id : \", parentId, \"Question : \",simQuestion)","metadata":{"_uuid":"33130dc1-3fe8-44db-9710-62c26684d1d4","_cell_guid":"caeed892-724b-444d-84a9-45279e1909f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getMostSimilarQuestions(5, \"How to MUltiply 2 columns pandas ?\", questions ,QIDList)","metadata":{"_uuid":"9ac75062-a319-4b8f-b54d-3b88a020e02b","_cell_guid":"11fe75c5-b2ab-4bfa-850a-d1c675ffab7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ask reddit troll questions**","metadata":{}},{"cell_type":"code","source":"reddit_troll = pd.read_csv(\"/kaggle/input/askreddit-troll-questions/our_competition_test.csv\")\nreddit_troll.columns","metadata":{"_uuid":"39aab02c-3bf6-4179-ac43-78f02bf39a5a","_cell_guid":"75d516d2-963c-4c13-9afd-494a0dd2b6ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"troll_questions = list(reddit_troll.question_text)\ntroll_questions[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_troll = pd.read_csv(\"/kaggle/input/askreddit-troll-questions/our_competition_train (1).csv\")\nreddit_troll.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"troll_questions.append(list(reddit_troll.question_text))\ntroll_questions[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Amazon questions answers**","metadata":{}},{"cell_type":"code","source":"amazon_questions = pd.read_csv(\"/kaggle/input/amazon-questionanswer-dataset/single_qna.csv\")\namazon_questions.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amazon_questions.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions = amazon_questions.Question[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers = amazon_questions.Answer[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qna = list(zip(questions, answers))\nqna[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_questions = pd.read_csv(\"/kaggle/input/amazon-questionanswer-dataset/multi_questions.csv\")\nmultiquestions = multi_questions.QuestionText\nlen(multiquestions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_answers = pd.read_csv(\"/kaggle/input/amazon-questionanswer-dataset/multi_answers.csv\")\nmultianswers = multi_answers.AnswerText\nlen(multianswers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Cleaning Text Data***","metadata":{}},{"cell_type":"code","source":"!pip install cleantext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cleantext\n\ncleaned_text = []\n\ndef clean(textdata, depth):\n    if depth == 1:\n        for i in textdata:\n            cleaned_text.append(cleantext.clean(str(i), extra_spaces=True, lowercase=True, stopwords=True, stemming=True, numbers=True, punct=True, clean_all = True))\n        \n        return cleaned_text[-10:]\n    else:\n        for i in textdata:\n            for j in i:\n                cleaned_text.append(cleantext.clean(str(j), extra_spaces=True, lowercase=True, stopwords=True, stemming=True, numbers=True, punct=True, clean_all = True))\n        \n        return cleaned_text[-10:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean(dialogs, depth = 1)\ndel dialogs\nclean(emapthetic_dialogs, depth = 1)\ndel emapthetic_dialogs\nclean(qna, depth = 2)\ndel qna\nclean(tc_dialogs, depth = 1)\ndel tc_dialogs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean(jqna, depth = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del jqna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean(troll_questions, depth=1)\ndel troll_questions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Getting all questions and answers in 1 variable**","metadata":{}},{"cell_type":"code","source":"all_questions = list(dialogs[::2]) + list(mh.Questions) + list(emapthetic_dialogs[::2]) + list(tc_dialogs[::2]) #+ list(jq[' Question']) #+ list(amazon_questions.Question) #+ list(multi_questions.QuestionText)\nall_answers = list(dialogs[1::2]) + list(mh.Answers) + list(emapthetic_dialogs[1::2]) + list(tc_dialogs[1::2]) #+ list(jq[' Answer']) #+ list(amazon_questions.Answer) #+ list(multi_answers.AnswerText)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_questions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Data training for chatbot***","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.keras import layers , activations , models , preprocessing , utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts([str(i) for i in all_questions[:1000]] +  [str(i) for i in all_answers[:1000]])\nVOCAB_SIZE = len( tokenizer.word_index )+1\nprint( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport re\n\nvocab = []\nfor word in tokenizer.word_index:\n    vocab.append( word )\n\ndef tokenize( sentences ):\n    tokens_list = []\n    vocabulary = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n        tokens = sentence.split()\n        vocabulary += tokens\n        tokens_list.append( tokens )\n    return tokens_list , vocabulary\n\n#p = tokenize( questions + answers )\n#model = Word2Vec( p[ 0 ] ) \n\n#embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )\n#for i in range( len( tokenizer.word_index ) ):\n    #embedding_matrix[ i ] = model[ vocab[i] ]\n\n# encoder_input_data\ntokenized_questions = tokenizer.texts_to_sequences( [str(i) for i in all_questions[:1000]] )\nmaxlen_questions = max( [ len(x) for x in tokenized_questions ] )\npadded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\nencoder_input_data = np.array( padded_questions )\nprint( encoder_input_data.shape , maxlen_questions )\n\n# decoder_input_data\ntokenized_answers = tokenizer.texts_to_sequences( [str(i) for i in all_answers[:1000]] )\nmaxlen_answers = max( [ len(x) for x in tokenized_answers ] )\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\ndecoder_input_data = np.array( padded_answers )\nprint( decoder_input_data.shape , maxlen_answers )\n\n# decoder_output_data\ntokenized_answers = tokenizer.texts_to_sequences( [str(i) for i in all_answers[:1000]] )\nfor i in range(len(tokenized_answers)) :\n    tokenized_answers[i] = tokenized_answers[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\nonehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\ndecoder_output_data = np.array( onehot_answers )\nprint( decoder_output_data.shape )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nencoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\nencoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\nencoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\nencoder_states = [ state_h , state_c ]\n\ndecoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\ndecoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\ndecoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\ndecoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \noutput = decoder_dense ( decoder_outputs )\n\nmodel = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 ) \nmodel.save( 'model.h5' ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference_models():\n    \n    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n    \n    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n    \n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_embedding , initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = tf.keras.models.Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n    \n    return encoder_model , decoder_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef str_to_tokens( sentence : str ):\n    words = sentence.lower().split()\n    tokens_list = list()\n    for word in words:\n        tokens_list.append( tokenizer.word_index[ word ] ) \n    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nenc_model , dec_model = make_inference_models()\n\nfor _ in range(10):\n    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n    empty_target_seq = np.zeros( ( 1 , 1 ) )\n    empty_target_seq[0, 0] = tokenizer.word_index['start']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n        sampled_word = None\n        for word , index in tokenizer.word_index.items() :\n            if sampled_word_index == index :\n                decoded_translation += ' {}'.format( word )\n                sampled_word = word\n        \n        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n            stop_condition = True\n            \n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        states_values = [ h , c ] \n\n    print( decoded_translation )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}